{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6c6a5b-db63-457e-ac66-7dc6ccff42bc",
   "metadata": {},
   "source": [
    "StudentID: 23220020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e735280-ce14-4393-a474-80d4efc54d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45042319-c2e5-4143-866c-87bae5028213",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Data Collection </font>\n",
    "\n",
    "Collect a large and diverse textual dataset suitable for training word embeddings. Ensure that the dataset is preprocessed: remove special characters, lowercase all words, etc. \n",
    "\n",
    "### Text Cleaning\n",
    "`text cleaning` -> `utf-coversion` -> `case normalisation` -> `contraction expansion` -> `tokenisation` -> `removing punctuations` -> `removing stopwords`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56483abe-b2eb-4eb1-9485-378084761872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "def tokenise_sentences(text):\n",
    "    #convert utf-8 characters to normal characters\n",
    "    text = convert_utf(text)\n",
    "    \n",
    "    #convert to lowercase\n",
    "    normalized_content = text.lower()\n",
    "    \n",
    "    #fix contractions\n",
    "    expanded_text = contractions.fix(normalized_content)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(expanded_text)\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "\n",
    "        #remove punctuations\n",
    "        #nltk.download(\"punkt\")\n",
    "        words_without_punctuations = []\n",
    "        more_punctuation = [\"''\",\"--\",\"``\"]\n",
    "        for x in words:\n",
    "            if x not in string.punctuation and x not in more_punctuation:\n",
    "                words_without_punctuations.append(x)\n",
    "\n",
    "        #remove stopwords\n",
    "        #nltk.download('stopwords')\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        filtered_tokens = []\n",
    "        for token in words_without_punctuations:\n",
    "            if token not in stop_words:\n",
    "                filtered_tokens.append(token)\n",
    "             \n",
    "\n",
    "        data.append(filtered_tokens)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acef97-ed34-434e-9146-f4dd73b25657",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Training </font>\n",
    "-  Use a `Word2Vec` embeddings technique. \n",
    "- Utilise Gensim library to assist with the training.\n",
    "- Save the trained model for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad158c12-c613-4b6c-b02f-8a0132d8fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=200, min_count=1, sg=0)\n",
    "model.save(\"./model_test\")\n",
    "\n",
    "with open(\"./dickens/924-0.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = tokenise_sentences(content)\n",
    "\n",
    "model.build_vocab(data, update=False)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')\n",
    "\n",
    "with open(\"./dickens/pg1392.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = tokenise_sentences(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')\n",
    "\n",
    "with open(\"./dickens/580-0.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = tokenise_sentences(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')\n",
    "\n",
    "with open(\"./dickens/pg1407.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = tokenise_sentences(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')\n",
    "\n",
    "\n",
    "file_names = [\"700-0.txt\", \"650-0.txt\", \"pg23344.txt\", \"98-0.txt\", \"967-0.txt\", \"963-0.txt\", \"914-0.txt\", \"pg730.txt\",\n",
    "\"1289-0.txt\", \"653-0.txt\", \"27924-0.txt\", \"1400-0.txt\", \"pg676.txt\", \"766-0.txt\", \"pg1023.txt\", \"882-0.txt\", \"644-0.txt\", \"pg699.txt\", \n",
    "\"675-0.txt\", \"807-0.txt\", \"786-0.txt\", \"564-0.txt\", \"pg32241.txt\", \"678-0.txt\", \"883-0.txt\", \"pg19337.txt\", \"917-0.txt\", \"1467-0.txt\"]\n",
    "for file_name in file_names:\n",
    "    with open(\"./dickens/\"+file_name, \"r\", encoding=\"utf8\") as file:    \n",
    "        content = file.read()\n",
    "\n",
    "    data = tokenise_sentences(content)\n",
    "\n",
    "    model.build_vocab(data, update=True)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save('./model_test')\n",
    "    print(\"Finished training on \" + file_name)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa39d3-9115-4cf1-8653-b154919ae7c2",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Web Application </font>\n",
    "- Design a simple web interface where a user can input a word. \n",
    "- Implement back-end functionality to fetch the opposite of the given word\n",
    "using the trained embeddings. \n",
    "- Return the opposite word to the user on the web interface.\n",
    "- Use `Flask` library for the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc93c1d-0156-4236-aaf5-80d028ebadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "app = Flask(__name__)\n",
    "\n",
    "html_form_with_message = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Antonym App</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2 style=\"color:pink\">sums</h2>\n",
    "    <form method=\"post\" action=\"/\">\n",
    "        <label for=\"text\">your word:</label><br>\n",
    "        <input type=\"text\" name=\"my_word\"><br><br>\n",
    "        <input type=\"submit\" value=\"My Button\">\n",
    "    </form>\n",
    "    <p>result</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "def my_antonym(target_word):\n",
    "    return model.wv.most_similar(positive=['woman', target_word], negative=['man']) #[1][2]\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    word_input = ''\n",
    "    opposite_value = ''\n",
    "    if request.method == 'POST':\n",
    "        word_input = request.form['my_word']\n",
    "        opposite_value = my_antonym(word_input)\n",
    "\n",
    "    if(len(opposite_value) > 1):\n",
    "        return html_form_with_message.replace(\"result\", f\"the opposite of {word_input} is: {str(opposite_value[0][0])} or {str(opposite_value[1][0])} or {str(opposite_value[2][0])}\")\n",
    "\n",
    "    return html_form_with_message\n",
    "    \n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534eb2e1-92c3-41a5-b5c8-80a803bb1138",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> References </font>\n",
    "- [1] https://research.wdss.io/word2vec/#Application\n",
    "- [2] https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca0d9f-424c-4552-9e57-0e890e205dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
